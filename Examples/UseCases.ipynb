{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to PyDKPro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to build [DKPro Core](https://dkpro.github.io/dkpro-core/) based pipelines, their processing using input strings or files, text annotation and how to use individual DKPro Core components. We also demonstrate interfacing of [spaCy](https://spacy.io/) and [nltk](http://www.nltk.org/) (python based nlp toolkits) with DKPro cas objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing PyDKPro\n",
    "\n",
    "PyDKPro supports Python 3.6 and above and uses [Docker](https://www.docker.com/) container which hosts web services for all Java based DKPro Core operation. To use this demo, make sure Python>=3.6 and pip package is installed. Please perform following steps (Step 1 is optional) in your terminal.\n",
    "\n",
    "1. To create vitual enviorment, \n",
    "\n",
    "    `python -m pip install virtualenv` (Install virtualenv if not already installed)\n",
    "\n",
    "    `mkdir [env_name]`\n",
    "    \n",
    "    `virtualenv -p python3 [env_name]` or `python3 -m venv [env_name]`\n",
    "    \n",
    "    To activate the environment,\n",
    "    \n",
    "    On Windows, run:\n",
    "\n",
    "    `[env_name]\\Scripts\\activate.bat`\n",
    "\n",
    "    On Unix or MacOS, run:\n",
    "\n",
    "    `source [env_name]/bin/activate`\n",
    "\n",
    "    If you want to create conda (version 4.6 or later) environment,\n",
    "\n",
    "    `conda create --name [env_name] python=3.6`\n",
    "    \n",
    "    To activate the environment (on Windows, MacOS and Unix),\n",
    "    \n",
    "    `conda activate [env_name]`\n",
    "    \n",
    "    \n",
    "\n",
    "2. Install latest python libraries: \n",
    "   \n",
    "   `python -m pip install -r requirements.txt`\n",
    "   \n",
    "   `python -m spacy download en_core_web_sm`\n",
    "   \n",
    "   \n",
    "\n",
    "3. Clone the repository.\n",
    "\n",
    "    `git clone https://github.com/zesch/pydkpro.git`\n",
    "    \n",
    "    \n",
    "\n",
    "4. Open this jupyter notebook in your browser.\n",
    "\n",
    "    `jupyter notebook`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies are installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to make sure that all dependencies are installed successfully.\n",
    "\n",
    "import pkg_resources\n",
    "from pkg_resources import DistributionNotFound, VersionConflict\n",
    "\n",
    "dependencies = [\n",
    "  'dkpro-cassis==0.2.7',\n",
    "  'spacy==2.2.1',\n",
    "  'nltk==3.4.5'\n",
    "]\n",
    " \n",
    "uninstalled_libraries = []\n",
    "for each_dependency in dependencies:\n",
    "    try:\n",
    "        pkg_resources.require([each_dependency])\n",
    "    except (DistributionNotFound, VersionConflict):\n",
    "        uninstalled_libraries.append(each_dependency)\n",
    "if len(uninstalled_libraries) > 0:\n",
    "    print(\" %s dependecies are not installed properly. Please install them and rerun this jupyter notebook\" \n",
    "          %(','.join(uninstalled_libraries)))\n",
    "else:\n",
    "    print(\"All dependencies are installed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Pipeline\n",
    "\n",
    "To process a piece of text, you'll need to first construct a `Pipeline` with different `DKPro Core` components. The pipeline is language-specific, so you'll need to first specify the language (see examples).\n",
    "\n",
    "- By default, the components of pipeline will include default parameters. However, you can always specify what parameters you want to include or change. Component parameter list is provided in [DKPro Core](https://dkpro.github.io/dkpro-core/) documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container web service for the provided pipeline is fired up. To stop use finish method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pydkpro.pipeline.Pipeline at 0x10cf17f90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydkpro import Pipeline, Component  \n",
    "p = Pipeline(version=\"2.0.0\", language='en')\n",
    "p.add(Component().clearNlpSegmenter())\n",
    "p.add(Component().stanfordPosTagger(variant='fast-caseless', printTagSet='true'))\n",
    "p.build() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline processing\n",
    "\n",
    "After pipeline construction, you'll need to process/trigger the pipeline with the piece of text, you want to process (see example below). If language parameter is not provided, then language detector will be used to detect the language of text. The output of processed pipeline will be `CAS` object which is container for accessing linguistic annotations having DKPro Core defined typesystem. PyDKPro provide DKPro Core type systems which are used by `CAS` object to extract the annotations e.g. `tokens`, `sentence`, `pos tags`, `ner`, etc. based on defined pipeline structure. \n",
    "\n",
    "\n",
    "The examples below demostrate how to extract token text and pos tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process('Backgammon is one of the oldest known board games.', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydkpro import DKProCoreTypeSystem as dts\n",
    "\n",
    "ts_token = dts().token\n",
    "\n",
    "cas.select(ts_token).as_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(ts_token).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Annotations\n",
    "\n",
    "Similar to [DKPro cassis](https://github.com/dkpro/dkpro-cassis), to add manual annotations to cas object, we need to defined it with `typesystem`. For the given text, annotations of Tokens that has an id and pos feature can be added in the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.cas import Cas\n",
    "Token = dts().typesystem.get_type('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token')\n",
    "cas = Cas(dts().typesystem)()\n",
    "cas.sofa_string = \"I like cheese .\"\n",
    "tokens = [\n",
    "    Token(begin=0, end=1, id='0', pos='NNP'),\n",
    "    Token(begin=2, end=6, id='1', pos='VBD'),\n",
    "    Token(begin=7, end=13, id='2', pos='IN'),\n",
    "    Token(begin=14, end=15, id='3', pos='.')\n",
    "]\n",
    "\n",
    "\n",
    "for token in tokens:\n",
    "    cas.add_annotation(token)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token features can printed as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'cheese', '.']\n",
      "['NNP', 'VBD', 'IN', '.']\n"
     ]
    }
   ],
   "source": [
    "print([x.get_covered_text() for x in cas.select_all()])\n",
    "print([x.pos for x in cas.select_all()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy interfacing\n",
    "\n",
    "Generated CAS objects can also be typecast to the spaCy annotation object model and vice-versa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro import To_spacy, From_spacy\n",
    "cas = p.process('Backgammon is one of the oldest known board games.', language='en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon NNP\n",
      "is VBZ\n",
      "one CD\n",
      "of IN\n",
      "the DT\n",
      "oldest JJS\n",
      "known VBN\n",
      "board NN\n",
      "games NNS\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in To_spacy(cas)(): \n",
    "    print(token.text, token.tag_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "cas = From_spacy(doc)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token()).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK interfacing\n",
    "\n",
    "Similar to spaCy, `NLTK` objects can also be convert into cas and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.external import To_nltk, From_nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to NLTK -  Since this toolkit doesn't have common dataset, PyDKPro provide helper functions e.g. `tagger` (see below example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Backgammon', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('oldest', 'JJS'),\n",
       " ('known', 'VBN'),\n",
       " ('board', 'NN'),\n",
       " ('games', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "To_nltk().tagger(cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(To_nltk().tagger(cas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Backgammon/NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  oldest/JJS\n",
      "  known/VBN\n",
      "  board/NN\n",
      "  games/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar helper functions are developed for NLTK to PyDKpro's CAS conversion as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "cas = From_nltk().tokenizer(TweetTokenizer().tokenize('Backgammon is one of the oldest known board games.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas processing\n",
    "\n",
    "PyDKPro pipeline also provide direct cas object processing as demonstrated in below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container web service for the provided pipeline is fired up. To stop use finish method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pydkpro.pipeline.Pipeline at 0x1309111d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline()\n",
    "p.add(Component().stanfordPosTagger())\n",
    "p.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process(cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).as_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut for running single components\n",
    "\n",
    "A single component can also be run without the need to build a pipeline first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Component().clearNlpSegmenter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas = tokenizer.process('Backgammon is one of the oldest known board games.')\n",
    "cas.select(dts().token).as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with list of strings\n",
    "\n",
    "Multiple strings in the form of list can also be processed, where each element of list will be considered as document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = ['Backgammon is one of the oldest known board games.', 'I like playing cricket.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games.']\n",
      "['I', 'like', 'playing', 'cricket.']\n"
     ]
    }
   ],
   "source": [
    "for str in str_list:\n",
    "    cas = p.process(str)\n",
    "    print(cas.select(dts().token).as_text()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with text documents\n",
    "\n",
    "Pipelines can also be directly run on text documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.external import File2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process(File2str('test_data/input/test2.txt')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with multiple text documents\n",
    "\n",
    "Multiple documents can also be processed by providing documents path and document name matching patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents available at different path can be provided in list\n",
    "docs = ['test_data/input/1.txt', 'test_data/input/2.txt']\n",
    "for doc in docs:\n",
    "    p.process(File2str(doc)())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With following command pipeline's collection process will be completed (Alternatively, scope operator `with` can be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
