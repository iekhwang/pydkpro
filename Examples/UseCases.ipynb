{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to PyDKPro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to build [DKPro Core](https://dkpro.github.io/dkpro-core/) based pipelines, their processing using input strings or files, text annotation and how to use individual DKPro components. We also demonstrate interfacing of [spaCy](https://spacy.io/) and [nltk](http://www.nltk.org/) (python based nlp toolkits) with DKPro cas objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing PyDKPro\n",
    "\n",
    "PyDKPro supports Python 3.6 and above and uses [Docker](https://www.docker.com/) container which hosts web services for all Java based DKPro core operation. To use this demo, perform following steps\n",
    "\n",
    "1. install latest python libraries: \n",
    "   \n",
    "   `pip install dkpro-cassis==0.2.7 spacy==2.2.1 nltk==3.4.5`\n",
    "   \n",
    "\n",
    "2. clone the repository\n",
    "\n",
    "3. open this jupyter notebook and simply run the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Pipeline\n",
    "\n",
    "To process a piece of text, you'll need to first construct a `Pipeline` with different `DKPro core` components. The pipeline is language-specific, so you'll need to first specify the language (see examples).\n",
    "\n",
    "- By default, the components of pipeline will include default parameters. However, you can always specify what parameters you want to include or change.Component parameter list will be provided in `PyDKPro` documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container web service for the provided pipeline is fired up. To stop use finish method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pydkpro.pipeline.Pipeline at 0x1190322d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydkpro import Pipeline, Component  \n",
    "p = Pipeline(version=\"2.0.0\", language='en')\n",
    "p.add(Component().clearNlpSegmenter())\n",
    "p.add(Component().stanfordPosTagger(variant='fast-caseless', printTagSet='false'))\n",
    "p.build() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline processing\n",
    "\n",
    "After pipeline construction, you'll need to process/trigger the pipeline with the piece of text, you want to process (see example below). If language parameter is not provided, then language detector will be used to detect the language of text. The output of processed pipeline will be `cas` object which is container for accessing linguistic annotations having DKPro defined typesystem. PyDKPro provide DKPro Core typesystems which is used by cas object to extract the annotations e.g. `tokens`, `sentence`, `pos tags`, `ner`, etc. based on defined pipeline structure. \n",
    "\n",
    "Please find below examples which demostrate, how to extract token text and pos tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process('Backgammon is one of the oldest known board games.', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydkpro import DKProCoreTypeSystem as dts\n",
    "\n",
    "cas.select(dts().token).as_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Annotations\n",
    "\n",
    "Similar to [DKPro cassis](https://github.com/dkpro/dkpro-cassis), to add manual annotations to cas object, we need to defined it with `typesystem`. For the given text, annotations of Tokens that has an id and pos feature can be added in the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.cas import Cas\n",
    "Token = dts().typesystem.get_type('de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token')\n",
    "cas = Cas(dts().typesystem)()\n",
    "cas.sofa_string = \"I like cheese .\"\n",
    "tokens = [\n",
    "    Token(begin=0, end=1, id='0', pos='NNP'),\n",
    "    Token(begin=2, end=6, id='1', pos='VBD'),\n",
    "    Token(begin=7, end=13, id='2', pos='IN'),\n",
    "    Token(begin=14, end=15, id='3', pos='.')\n",
    "]\n",
    "\n",
    "\n",
    "for token in tokens:\n",
    "    cas.add_annotation(token)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cas token attributes can printed as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'cheese', '.']\n",
      "['NNP', 'VBD', 'IN', '.']\n"
     ]
    }
   ],
   "source": [
    "print([x.get_covered_text() for x in cas.select_all()])\n",
    "print([x.pos for x in cas.select_all()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy interfacing\n",
    "\n",
    "Generated CAS objects can also be typecast to the spaCy type system and vice-versa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro import To_spacy, From_spacy\n",
    "cas = p.process('Backgammon is one of the oldest known board games.', language='en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon NNP\n",
      "is VBZ\n",
      "one CD\n",
      "of IN\n",
      "the DT\n",
      "oldest JJS\n",
      "known VBN\n",
      "board NN\n",
      "games NNS\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in To_spacy(cas)(): \n",
    "    print(token.text, token.tag_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "cas = From_spacy(doc)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token()).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK interfacing\n",
    "\n",
    "Similar to spaCy, `NLTK` objects can also be convert into cas and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.external import To_nltk, From_nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to NLTK -  Since this toolkit doesn't have common dataset, PyDKPro provide helper functions e.g. `tagger` (see below example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Backgammon', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('oldest', 'JJS'),\n",
       " ('known', 'VBN'),\n",
       " ('board', 'NN'),\n",
       " ('games', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "To_nltk().tagger(cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(To_nltk().tagger(cas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Backgammon/NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  oldest/JJS\n",
      "  known/VBN\n",
      "  board/NN\n",
      "  games/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar helper functions are developed for NLTK to PyDKpro's cas conversion as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "cas = From_nltk().tokenizer(TweetTokenizer().tokenize('Backgammon is one of the oldest known board games.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas processing\n",
    "\n",
    "PyDKPro pipeline also provide direct cas object processing as demonstrated in below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container web service for the provided pipeline is fired up. To stop use finish method\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pydkpro.pipeline.Pipeline at 0x129cadb10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline()\n",
    "p.add(Component().stanfordPosTagger())\n",
    "p.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process(cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).as_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP', 'VBZ', 'NN', 'IN', 'DT', 'JJS', 'VBN', 'NN', 'NNS', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token).get_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut for running single components\n",
    "\n",
    "A single component can also be run without the need to build a pipeline first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Component().clearNlpSegmenter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas = tokenizer.process('Backgammon is one of the oldest known board games.')\n",
    "cas.select(dts().token).as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with list of strings\n",
    "\n",
    "Multiple strings in the form of list can also be processed, where each element of list will be considered as document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = ['Backgammon is one of the oldest known board games.', 'I like playing cricket.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games.']\n",
      "['I', 'like', 'playing', 'cricket.']\n"
     ]
    }
   ],
   "source": [
    "for str in str_list:\n",
    "    cas = p.process(str)\n",
    "    print(cas.select(dts().token).as_text()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with text documents\n",
    "\n",
    "Pipelines can also be directly run on text documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydkpro.external import File2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cas = p.process(File2str('test_data/input/test2.txt')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cas.select(dts().token.as_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with multiple text documents\n",
    "\n",
    "Multiple documents can also be processed by providing documents path and document name matching patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents available at different path can be provided in list\n",
    "docs = ['test_data/input/1.txt', 'test_data/input/2.txt']\n",
    "for doc in docs:\n",
    "    p.process(File2str(doc)())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End collection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With following command pipeline's collection process will be completed (Alternatively, scope operator `with` can be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
